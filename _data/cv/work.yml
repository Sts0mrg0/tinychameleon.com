- title: Staff Software Engineer, Techncial Lead
  place: Kixeye
  location: Victoria, BC
  timeframe:
    start: October 2018
    end: Present
  description: >-
    After doing what I consider to be great work, I was promoted and I also decided to take
    on the technical lead responsibilities for the team during a time of change at the company.
    My day-to-day still involves delivering high-value software solutions to problems the company
    needs solved, but I also frequently help others on the team.

    I spent a good deal of time reviewing technical documents to ensure the long-term maintainability
    of systems we build meets standards to sets the company up for future success.
    I mentor team-mates, so that they can continue to improve at programming and in their daily utilization
    of the tools we employ.
    I keep tabs on individual ticket backlogs so I have a good high-level overview of what is in-flight
    at any given time, and importantly, what can be shelved when priority-one problems appear.

    I do enjoy the leadership role because though I am capable of delivering high-value solutions,
    there's only one of me.
    By mentoring and illuminating better technical paths, I can improve everyone around me which is
    far better for the company than simply having one of me working on projects.

    Many of my technical projects at this stage involve higher-level deconstructions of older solutions;
    things like properly decommissioning aging systems without incurring game outages.

- title: Senior Software Engineer
  place: Kixeye
  location: Victoria, BC
  timeframe:
    start: November 2016
    end: October 2018
  description: >-
    I made a fairly large pivot into the video game space and simultaneously jumped into working
    remotely at this point.
    Joining what was originally the analytics engineering team, but quickly became the central
    operations team for the entire company, I was exposed to a wide variety of projects.
    These ranged from creating and maintaining ETL systems backing the data warehouse analysts used
    in their day-to-day work to large-scale web properties.

    These diverse projects cover a range of technologies, like Python, PHP, Javascript, and SQL, but
    also a range of different historical approaches taken by different teams.
    Legacy applications live side-by-side with more modern approaches because the ROI for rewrites
    simply doesn't exist when the legacy applications work just fine.

    One of the projects I have put large amounts of work into is the event ingestion system -- an RPC
    API all of the games utilize to record analytics events.
    I put a large amount of effort was put into creating unit and integration tests for this system that
    had none.
    More effort was channelled into creating proper disaster recovery and deployment processes involving
    replacing most of the unreliable shell scripts with things like CloudFormation.
    The deployment process for this system, and its operation, are now zero-downtime; the system operates
    in a high-availability manner, servicing around 2 billion events per day.

    Another component I've worked heavily with is the data warehousing solution, which currently contains
    massive volumes of data.
    This data warehouse is used by all game teams and requires constant optimization to maintain reasonable
    operational costs.
    I've been directly involved in optimiztion of costs and have reduced said costs by approximately 15%
    in the past, yielding per-year savings in the hundreds-of-thousands-of-dollars range.

    Finally, the last piece of work I will mention is offer targetting -- every mobile game tends to have
    offers presented to users, either as free items or for purchase.
    I created a system that allows analysts to use SQL to define named partitions of users which can be
    updated on a schedule.
    This system is capable of running thousands of partitioning queries concurrently, loading and updating
    millions of rows per minute, while simultaneously keeping an accurate historical log of user membership
    across all named partitions.
    This high-volume update system powers nearly all the offers players see.

    Aside from those more detailed projects, there are plenty of other things I do day-to-day -- far too
    many to really mention here -- but because I am on the central operations team, I basically touch
    every system that's critical to the company.
    Things like JIRA, SCM servers, and game infrastructure all fall under my responsibility; of course
    this also means that I am part of an on-call rotation for responding to system failures.

- title: Software Engineer
  place: Vendasta
  location: Saskatoon, SK
  timeframe:
    start: August 2014
    end: November 2016
  description: >-
    I don't make it much of a secret that I believe I should be compensated for my time and ability,
    so in August 2014 I switched roles to a position at a then still quite small company called VendAsta.
    They subsequently changed the formatting of the name to remove the uppercase 'A', which I think was
    a great idea.

    During my time here I worked with some amazing people building advertising and social media management
    tools for large media chains to effectively sell services to Small-to-Medium Businesses (SMBs).
    Essentially, we provided tools like Social Marketing, in this case that helped a business manage posting
    to many social media platforms, which larger media companies could resell to SMBs.
    All of these tools are white-labelled, meaning our customers were capable of branding them as their
    product, and none of the SMBs really knew we existed in their tool chain.

    Most of these products were hosted using the original version of Google AppEngine, which was a godsend
    for aleviating operational responsibility at the time, for the company's size.
    Lots of back-end Python work, and front-end Javascript work in Knockout.js and then eventually the
    original Angular.js, across numerous products.
    It was at this job that I ended up becoming intimately familiar with the ins-and-outs of the less visited
    corners of Python and Javascript.

    I gained familiarity with ElasticSearch, memcached-Ã -la-AppEngine, and a variety of other Google-created
    pieces of technology, particularly when related to distributed, transactional tasks.
    Google's Cloud PubSub had become the hot, new thing, and all our services began to chat in faux real-time
    using it.

    One of the most important parts of my stay at this company was being introduced to the idea of having
    dedicated quality assurance resources.
    Aside from the soft-skills benefit of having to explain yourself to another person, the larger benefit
    is having someone who knows the product and is acting in an adversarial role.
    As a programmer you will improve quite rapidly when a good quality assurance person is constantly breaking
    your code in numerous ways.

- title: Software Engineer
  place: Vecima Networks
  location: Saskatoon, SK
  timeframe:
    start: September 2012
    end: July 2014
  description: >-
    My work at Vecima Networks revolved around a piece of fleet tracking software called FleetLynx.
    The main thrust of the system was to visualize and display fleet and truck data for customers,
    including data from interfacing with standard vehicle buses like SAE J1939 and SAE J1979.

    I began by working on the original version, but quickly was placed in charge of creating a better
    version 2 of the software.
    The original version was written using early variants of Backbone.js, with many areas that could
    be improved upon.
    I spent time creating the new version using Ember.js, which is still in use today, and is a decision
    I do not regret as the Ember.js team has shown an impressive commitment to version compatibility.

    The backend ran using early node.js versions between 0.9 and 0.10 and utilized AWS Beanstalk to avoid
    burdening the very small team with operational concerns.
    It was a fairly simple RPC service that received data from the on-board hardware, did a little
    post-processing, and stored the data in DynamoDB.
    There were a few endpoints exposed to access the data for visualization, and it was an all-around
    simple solution which was very easy to maintain.

    Much of the complexity involved liaison with the embedded programmers to ensure that both sides
    had a clear understanding of what data needed to be pulled off of the vehicle bus and sent, but
    more importantly, what data could not be dropped.

    During this time period I had fantastic performance reviews stating that it would be difficult to
    find someone with five years of experience that could accomplish what I did.
